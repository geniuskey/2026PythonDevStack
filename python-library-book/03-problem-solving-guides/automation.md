# ìë™í™” ì‹œìŠ¤í…œ êµ¬ì¶•

## ëª©í‘œ

ë°˜ë³µ ì‘ì—…ì„ ìë™í™”í•˜ì—¬ ìƒì‚°ì„±ì„ í–¥ìƒì‹œí‚¤ê³ , ì•ˆì •ì ì´ê³  í™•ì¥ ê°€ëŠ¥í•œ ìë™í™” ì‹œìŠ¤í…œ êµ¬ì¶•í•˜ê¸°

---

## ìë™í™” ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜

```mermaid
graph TD
    A[íŠ¸ë¦¬ê±°] --> B{ì‹¤í–‰ ë°©ì‹}
    B -->|ìŠ¤ì¼€ì¤„| C[ì‹œê°„ ê¸°ë°˜]
    B -->|ì´ë²¤íŠ¸| D[íŒŒì¼/ì›¹í›…]
    B -->|ìˆ˜ë™| E[CLI ëª…ë ¹]

    C --> F[ì‘ì—… ì‹¤í–‰]
    D --> F
    E --> F

    F --> G[ë¡œê¹…]
    F --> H[ì•Œë¦¼]
    F --> I[ê²°ê³¼ ì €ì¥]

    style F fill:#4CAF50
    style G fill:#FF9800
    style H fill:#2196F3
```

---

## ê¸°ìˆ  ìŠ¤íƒ

| ì‹œë‚˜ë¦¬ì˜¤ | ë„êµ¬ | ì‚¬ìš© ì˜ˆì‹œ | 2026 ê¶Œì¥ |
|----------|------|----------|-----------|
| **ìŠ¤ì¼€ì¤„ë§** | APScheduler | ë§¤ì¼ ì•„ì¹¨ ë¦¬í¬íŠ¸ ìƒì„± | - 2026 ê¶Œì¥: |
| **CLI ë„êµ¬** | click / typer | ëª…ë ¹ì¤„ ì¸í„°í˜ì´ìŠ¤ | - 2026 ê¶Œì¥: |
| **ì›¹ ìŠ¤í¬ë˜í•‘** | httpx + bs4 | ë°ì´í„° ìˆ˜ì§‘ | - 2026 ê¶Œì¥: |
| **ë¸Œë¼ìš°ì € ìë™í™”** | playwright | ì›¹ í…ŒìŠ¤íŠ¸, í¬ë¡¤ë§ | - 2026 ê¶Œì¥: |
| **íŒŒì¼ ê°ì‹œ** | watchdog | íŒŒì¼ ë³€ê²½ ì‹œ ìë™ ì²˜ë¦¬ | - 2026 ê¶Œì¥: |
| **ì‘ì—… í** | dramatiq | ë¹„ë™ê¸° ì‘ì—… ì²˜ë¦¬ | - 2026 ê¶Œì¥: |
| **ì›ê²© ì‹¤í–‰** | fabric | ì›ê²© ì„œë²„ ì‘ì—… | - |
| **ì›Œí¬í”Œë¡œìš°** | Dagster | ë³µì¡í•œ ìë™í™” ì²´ì¸ | - 2026 ê¶Œì¥: |

---

## í”„ë¡œì íŠ¸ êµ¬ì¡°

```
automation-system/
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ .env
â”œâ”€â”€ config/
â”‚   â””â”€â”€ settings.py
â”œâ”€â”€ tasks/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ scheduler.py      # ìŠ¤ì¼€ì¤„ ì‘ì—…
â”‚   â”œâ”€â”€ scraper.py        # ì›¹ ìŠ¤í¬ë˜í•‘
â”‚   â”œâ”€â”€ file_watcher.py   # íŒŒì¼ ê°ì‹œ
â”‚   â””â”€â”€ notifications.py  # ì•Œë¦¼
â”œâ”€â”€ cli/
â”‚   â””â”€â”€ main.py           # CLI ì§„ì…ì 
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ logger.py
â”‚   â””â”€â”€ helpers.py
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ input/
â”‚   â””â”€â”€ output/
â””â”€â”€ logs/
```

### ì˜ì¡´ì„± ì„¤ì¹˜

```bash
$ uv init automation-system
$ cd automation-system

# í•µì‹¬ ë¼ì´ë¸ŒëŸ¬ë¦¬
$ uv add apscheduler click httpx beautifulsoup4

# ë¸Œë¼ìš°ì € ìë™í™”
$ uv add playwright
$ uv run playwright install

# íŒŒì¼ ê°ì‹œ
$ uv add watchdog

# ì‘ì—… í
$ uv add dramatiq redis

# ì•Œë¦¼
$ uv add slack-sdk

# ë¡œê¹…
$ uv add structlog

# ê°œë°œ ë„êµ¬
$ uv add --dev pytest pytest-asyncio
```

---

## ìŠ¤ì¼€ì¤„ë§ ìë™í™”

```mermaid
graph LR
    A[APScheduler] --> B[Cron-like]
    A --> C[Interval]
    A --> D[Date-based]

    B --> E[ì‘ì—… ì‹¤í–‰]
    C --> E
    D --> E

    E --> F[ë¡œê·¸]
    E --> G[ì•Œë¦¼]

    style A fill:#9C27B0
    style E fill:#4CAF50
```

### íŒ¨í„´ 1: ê¸°ë³¸ ìŠ¤ì¼€ì¤„ë§

```python
# tasks/scheduler.py
from apscheduler.schedulers.blocking import BlockingScheduler
from apscheduler.triggers.cron import CronTrigger
from apscheduler.triggers.interval import IntervalTrigger
import structlog
from datetime import datetime

logger = structlog.get_logger()

class TaskScheduler:
    def __init__(self):
        self.scheduler = BlockingScheduler()
        self._setup_jobs()

    def _setup_jobs(self):
        """ì‘ì—… ë“±ë¡"""

        # ë§¤ì¼ ì˜¤ì „ 9ì‹œ
        self.scheduler.add_job(
            self.daily_report,
            trigger=CronTrigger(hour=9, minute=0),
            id='daily_report',
            name='ì¼ì¼ ë¦¬í¬íŠ¸ ìƒì„±'
        )

        # 1ì‹œê°„ë§ˆë‹¤
        self.scheduler.add_job(
            self.hourly_check,
            trigger=IntervalTrigger(hours=1),
            id='hourly_check',
            name='ì‹œê°„ë³„ ì²´í¬'
        )

        # ë§¤ì£¼ ì›”ìš”ì¼ ì˜¤ì „ 10ì‹œ
        self.scheduler.add_job(
            self.weekly_summary,
            trigger=CronTrigger(day_of_week='mon', hour=10, minute=0),
            id='weekly_summary',
            name='ì£¼ê°„ ìš”ì•½'
        )

        # íŠ¹ì • ë‚ ì§œ/ì‹œê°„ (ì¼íšŒì„±)
        self.scheduler.add_job(
            self.one_time_task,
            trigger='date',
            run_date=datetime(2026, 12, 31, 23, 59, 59),
            id='year_end_task'
        )

    def daily_report(self):
        """ì¼ì¼ ë¦¬í¬íŠ¸ ìƒì„±"""
        logger.info("daily_report_started")

        try:
            # ë°ì´í„° ìˆ˜ì§‘
            data = self._collect_daily_data()

            # ë¦¬í¬íŠ¸ ìƒì„±
            report = self._generate_report(data)

            # ì €ì¥
            self._save_report(report, f"daily_report_{datetime.now():%Y%m%d}.pdf")

            # ì•Œë¦¼ ì „ì†¡
            self._send_notification("ì¼ì¼ ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ")

            logger.info("daily_report_completed")

        except Exception as e:
            logger.error("daily_report_failed", error=str(e))
            self._send_alert(f"ì¼ì¼ ë¦¬í¬íŠ¸ ì‹¤íŒ¨: {e}")

    def hourly_check(self):
        """ì‹œê°„ë³„ ì‹œìŠ¤í…œ ì²´í¬"""
        logger.info("hourly_check_started")

        # ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸
        # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í™•ì¸
        # ë””ìŠ¤í¬ ê³µê°„ í™•ì¸
        # ...

        logger.info("hourly_check_completed")

    def weekly_summary(self):
        """ì£¼ê°„ ìš”ì•½"""
        logger.info("weekly_summary_started")

        # ì§€ë‚œ ì£¼ ë°ì´í„° ì§‘ê³„
        # ì£¼ê°„ ë¦¬í¬íŠ¸ ìƒì„±
        # ì´ë©”ì¼ ë°œì†¡

        logger.info("weekly_summary_completed")

    def one_time_task(self):
        """ì¼íšŒì„± ì‘ì—…"""
        logger.info("one_time_task_executed")

    def start(self):
        """ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘"""
        logger.info("scheduler_started", jobs=len(self.scheduler.get_jobs()))

        try:
            self.scheduler.start()
        except (KeyboardInterrupt, SystemExit):
            logger.info("scheduler_stopped")

# ì‹¤í–‰
if __name__ == '__main__':
    scheduler = TaskScheduler()
    scheduler.start()
```

### íŒ¨í„´ 2: ë¹„ë™ê¸° ìŠ¤ì¼€ì¤„ë§

```python
from apscheduler.schedulers.asyncio import AsyncIOScheduler
import asyncio
import httpx

class AsyncTaskScheduler:
    def __init__(self):
        self.scheduler = AsyncIOScheduler()
        self.http_client = httpx.AsyncClient()

    async def fetch_api_data(self):
        """API ë°ì´í„° ìˆ˜ì§‘ (ë¹„ë™ê¸°)"""
        logger.info("fetching_api_data")

        endpoints = [
            '/api/users',
            '/api/orders',
            '/api/products'
        ]

        # ë³‘ë ¬ ìš”ì²­
        tasks = [self.http_client.get(f'https://api.example.com{ep}') for ep in endpoints]
        responses = await asyncio.gather(*tasks)

        for endpoint, response in zip(endpoints, responses):
            data = response.json()
            logger.info("api_data_fetched", endpoint=endpoint, records=len(data))

    async def process_queue(self):
        """í ì²˜ë¦¬"""
        logger.info("processing_queue")

        # Redis íì—ì„œ ì‘ì—… ê°€ì ¸ì˜¤ê¸°
        # ë¹„ë™ê¸° ì²˜ë¦¬
        # ...

    def start(self):
        """ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘"""
        # ì‘ì—… ë“±ë¡
        self.scheduler.add_job(
            self.fetch_api_data,
            trigger=IntervalTrigger(minutes=5),
            id='api_fetch'
        )

        self.scheduler.add_job(
            self.process_queue,
            trigger=IntervalTrigger(seconds=30),
            id='queue_process'
        )

        self.scheduler.start()

        # ì´ë²¤íŠ¸ ë£¨í”„ ìœ ì§€
        asyncio.get_event_loop().run_forever()
```

### íŒ¨í„´ 3: ì¡°ê±´ë¶€ ìŠ¤ì¼€ì¤„ë§

```python
class ConditionalScheduler:
    """ì¡°ê±´ì— ë”°ë¼ ì‘ì—… ì‹¤í–‰"""

    def __init__(self):
        self.scheduler = BlockingScheduler()

    def should_run_backup(self) -> bool:
        """ë°±ì—… ì‹¤í–‰ ì¡°ê±´ í™•ì¸"""
        # ì£¼ë§ì—ë§Œ ì‹¤í–‰
        now = datetime.now()
        if now.weekday() >= 5:  # í† , ì¼
            return True

        # ë””ìŠ¤í¬ ê³µê°„ í™•ì¸
        import shutil
        stat = shutil.disk_usage("/")
        free_gb = stat.free / (1024**3)

        return free_gb > 10  # 10GB ì´ìƒ ì—¬ìœ  ìˆì„ ë•Œë§Œ

    def conditional_backup(self):
        """ì¡°ê±´ë¶€ ë°±ì—…"""
        if not self.should_run_backup():
            logger.info("backup_skipped", reason="condition_not_met")
            return

        logger.info("backup_started")
        # ë°±ì—… ë¡œì§
        logger.info("backup_completed")

    def start(self):
        self.scheduler.add_job(
            self.conditional_backup,
            trigger=CronTrigger(hour=2, minute=0),  # ë§¤ì¼ ìƒˆë²½ 2ì‹œ ì²´í¬
            id='conditional_backup'
        )
        self.scheduler.start()
```

---

## CLI ìë™í™” ë„êµ¬

```mermaid
graph TD
    A[CLI ëª…ë ¹] --> B[click/typer]
    B --> C[ì¸ì íŒŒì‹±]
    C --> D[ìœ íš¨ì„± ê²€ì¦]
    D --> E[ì‘ì—… ì‹¤í–‰]
    E --> F[ê²°ê³¼ ì¶œë ¥]
    E --> G[ë¡œê·¸ ê¸°ë¡]

    style B fill:#FF6B6B
    style E fill:#4ECDC4
```

### íŒ¨í„´ 1: Click ê¸°ë°˜ CLI

```python
# cli/main.py
import click
from pathlib import Path
import structlog

logger = structlog.get_logger()

@click.group()
@click.option('--config', type=click.Path(exists=True), help='ì„¤ì • íŒŒì¼ ê²½ë¡œ')
@click.pass_context
def cli(ctx, config):
    """ìë™í™” CLI ë„êµ¬"""
    ctx.ensure_object(dict)
    ctx.obj['config'] = config

    if config:
        click.echo(f"ì„¤ì • íŒŒì¼ ë¡œë“œ: {config}")

@cli.command()
@click.option('--source', '-s', required=True, help='ì†ŒìŠ¤ ë””ë ‰í† ë¦¬')
@click.option('--dest', '-d', required=True, help='ëŒ€ìƒ ë””ë ‰í† ë¦¬')
@click.option('--pattern', default='*.csv', help='íŒŒì¼ íŒ¨í„´')
@click.option('--dry-run', is_flag=True, help='ì‹¤ì œ ì‹¤í–‰ ì—†ì´ ë¯¸ë¦¬ë³´ê¸°')
def sync(source, dest, pattern, dry_run):
    """íŒŒì¼ ë™ê¸°í™”"""

    source_path = Path(source)
    dest_path = Path(dest)

    if not source_path.exists():
        click.secho(f"- ë¯¸ì§€ì›: ì†ŒìŠ¤ ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤: {source}", fg='red')
        return

    files = list(source_path.glob(pattern))

    if dry_run:
        click.secho(f"ğŸ” ë¯¸ë¦¬ë³´ê¸° ëª¨ë“œ ({len(files)}ê°œ íŒŒì¼)", fg='yellow')
        for f in files:
            click.echo(f"  - {f.name}")
        return

    with click.progressbar(files, label='ë™ê¸°í™” ì¤‘') as bar:
        for file in bar:
            import shutil
            shutil.copy2(file, dest_path / file.name)

    click.secho(f"- {len(files)}ê°œ íŒŒì¼ ë™ê¸°í™” ì™„ë£Œ", fg='green')

@cli.command()
@click.argument('url')
@click.option('--output', '-o', type=click.File('w'), default='-')
@click.option('--format', type=click.Choice(['json', 'csv', 'text']), default='json')
def fetch(url, output, format):
    """URLì—ì„œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°"""

    import httpx

    with click.spinner('ë°ì´í„° ê°€ì ¸ì˜¤ëŠ” ì¤‘...'):
        response = httpx.get(url, timeout=30)

    if response.status_code != 200:
        click.secho(f"- ë¯¸ì§€ì›: HTTP {response.status_code}", fg='red')
        return

    if format == 'json':
        import json
        data = response.json()
        output.write(json.dumps(data, indent=2, ensure_ascii=False))
    elif format == 'text':
        output.write(response.text)

    click.secho(f"- ë°ì´í„° ì €ì¥ ì™„ë£Œ", fg='green')

@cli.command()
@click.option('--days', default=7, help='ì²˜ë¦¬í•  ì¼ìˆ˜')
@click.option('--verbose', '-v', is_flag=True, help='ìì„¸í•œ ì¶œë ¥')
def process(days, verbose):
    """ë°ì´í„° ì²˜ë¦¬"""

    if verbose:
        click.echo(f"ì²˜ë¦¬ ê¸°ê°„: ìµœê·¼ {days}ì¼")

    # ë°ì´í„° ì²˜ë¦¬ ë¡œì§
    import time

    with click.progressbar(range(days), label='ì²˜ë¦¬ ì¤‘') as bar:
        for day in bar:
            time.sleep(0.1)  # ì‹œë®¬ë ˆì´ì…˜

    click.secho("- ì²˜ë¦¬ ì™„ë£Œ", fg='green')

@cli.command()
def status():
    """ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸"""

    click.echo(click.style("ì‹œìŠ¤í…œ ìƒíƒœ", bold=True, fg='blue'))
    click.echo("â”€" * 40)

    # CPU
    import psutil
    cpu_percent = psutil.cpu_percent(interval=1)
    click.echo(f"CPU: {cpu_percent}%")

    # ë©”ëª¨ë¦¬
    mem = psutil.virtual_memory()
    click.echo(f"ë©”ëª¨ë¦¬: {mem.percent}% (ì‚¬ìš© ì¤‘: {mem.used / 1024**3:.1f}GB)")

    # ë””ìŠ¤í¬
    disk = psutil.disk_usage('/')
    click.echo(f"ë””ìŠ¤í¬: {disk.percent}% (ì—¬ìœ : {disk.free / 1024**3:.1f}GB)")

if __name__ == '__main__':
    cli()
```

### íŒ¨í„´ 2: Typer ê¸°ë°˜ CLI (íƒ€ì… ì•ˆì „)

```python
import typer
from typing import Optional, List
from pathlib import Path
from enum import Enum

app = typer.Typer()

class OutputFormat(str, Enum):
    JSON = "json"
    CSV = "csv"
    TEXT = "text"

@app.command()
def convert(
    input_file: Path = typer.Argument(..., help="ì…ë ¥ íŒŒì¼"),
    output_file: Optional[Path] = typer.Option(None, "--output", "-o", help="ì¶œë ¥ íŒŒì¼"),
    format: OutputFormat = typer.Option(OutputFormat.JSON, help="ì¶œë ¥ í˜•ì‹"),
    overwrite: bool = typer.Option(False, "--overwrite", help="ê¸°ì¡´ íŒŒì¼ ë®ì–´ì“°ê¸°"),
):
    """íŒŒì¼ í˜•ì‹ ë³€í™˜"""

    if not input_file.exists():
        typer.secho(f"- ë¯¸ì§€ì›: íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {input_file}", fg=typer.colors.RED)
        raise typer.Exit(1)

    if output_file and output_file.exists() and not overwrite:
        typer.secho(f"- ë ˆê±°ì‹œ:  íŒŒì¼ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤: {output_file}", fg=typer.colors.YELLOW)
        if not typer.confirm("ë®ì–´ì“°ì‹œê² ìŠµë‹ˆê¹Œ?"):
            raise typer.Exit()

    typer.echo(f"ë³€í™˜ ì¤‘: {input_file} â†’ {format.value}")

    # ë³€í™˜ ë¡œì§
    # ...

    typer.secho("- ë³€í™˜ ì™„ë£Œ", fg=typer.colors.GREEN)

@app.command()
def batch(
    files: List[Path] = typer.Argument(..., help="ì²˜ë¦¬í•  íŒŒì¼ë“¤"),
    workers: int = typer.Option(4, "--workers", "-w", help="ë³‘ë ¬ ì›Œì»¤ ìˆ˜"),
):
    """ì¼ê´„ ì²˜ë¦¬"""

    from concurrent.futures import ThreadPoolExecutor

    typer.echo(f"ì²˜ë¦¬í•  íŒŒì¼: {len(files)}ê°œ, ì›Œì»¤: {workers}ê°œ")

    with ThreadPoolExecutor(max_workers=workers) as executor:
        results = list(executor.map(process_file, files))

    typer.secho(f"- {len(results)}ê°œ íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ", fg=typer.colors.GREEN)

def process_file(file: Path):
    """íŒŒì¼ ì²˜ë¦¬"""
    # ì²˜ë¦¬ ë¡œì§
    pass

if __name__ == "__main__":
    app()
```

---

## ì›¹ ìŠ¤í¬ë˜í•‘ ìë™í™”

### íŒ¨í„´ 1: httpx + BeautifulSoup

```python
# tasks/scraper.py
import httpx
from bs4 import BeautifulSoup
import asyncio
from typing import List, Dict
import structlog

logger = structlog.get_logger()

class WebScraper:
    def __init__(self, base_url: str):
        self.base_url = base_url
        self.client = httpx.AsyncClient(
            timeout=30.0,
            headers={'User-Agent': 'Mozilla/5.0 (compatible; Bot/1.0)'}
        )

    async def scrape_page(self, url: str) -> Dict:
        """ë‹¨ì¼ í˜ì´ì§€ ìŠ¤í¬ë˜í•‘"""
        logger.info("scraping_page", url=url)

        response = await self.client.get(url)
        response.raise_for_status()

        soup = BeautifulSoup(response.text, 'html.parser')

        # ë°ì´í„° ì¶”ì¶œ
        data = {
            'title': soup.find('h1').text.strip() if soup.find('h1') else None,
            'content': soup.find('article').text.strip() if soup.find('article') else None,
            'links': [a['href'] for a in soup.find_all('a', href=True)],
            'images': [img['src'] for img in soup.find_all('img', src=True)],
        }

        return data

    async def scrape_multiple(self, urls: List[str]) -> List[Dict]:
        """ì—¬ëŸ¬ í˜ì´ì§€ ë³‘ë ¬ ìŠ¤í¬ë˜í•‘"""
        logger.info("scraping_multiple", count=len(urls))

        tasks = [self.scrape_page(url) for url in urls]
        results = await asyncio.gather(*tasks, return_exceptions=True)

        # ì—ëŸ¬ ì²˜ë¦¬
        success_count = sum(1 for r in results if not isinstance(r, Exception))
        logger.info("scraping_completed", success=success_count, total=len(urls))

        return [r for r in results if not isinstance(r, Exception)]

    async def scrape_with_pagination(self, start_url: str, max_pages: int = 10) -> List[Dict]:
        """í˜ì´ì§€ë„¤ì´ì…˜ ì²˜ë¦¬"""
        all_data = []
        current_url = start_url

        for page in range(1, max_pages + 1):
            logger.info("scraping_page", page=page, url=current_url)

            response = await self.client.get(current_url)
            soup = BeautifulSoup(response.text, 'html.parser')

            # ë°ì´í„° ì¶”ì¶œ
            items = soup.find_all('div', class_='item')
            for item in items:
                all_data.append({
                    'title': item.find('h2').text.strip(),
                    'price': item.find('span', class_='price').text.strip(),
                    'url': item.find('a')['href'],
                })

            # ë‹¤ìŒ í˜ì´ì§€ ì°¾ê¸°
            next_link = soup.find('a', class_='next')
            if not next_link:
                break

            current_url = next_link['href']

            # Rate limiting
            await asyncio.sleep(1)

        logger.info("pagination_complete", total_items=len(all_data))
        return all_data

    async def close(self):
        await self.client.aclose()

# ì‚¬ìš© ì˜ˆì‹œ
async def main():
    scraper = WebScraper("https://example.com")

    # ë‹¨ì¼ í˜ì´ì§€
    data = await scraper.scrape_page("https://example.com/article/123")

    # ì—¬ëŸ¬ í˜ì´ì§€
    urls = [f"https://example.com/article/{i}" for i in range(1, 11)]
    results = await scraper.scrape_multiple(urls)

    # í˜ì´ì§€ë„¤ì´ì…˜
    all_items = await scraper.scrape_with_pagination("https://example.com/products")

    await scraper.close()

if __name__ == '__main__':
    asyncio.run(main())
```

### íŒ¨í„´ 2: Playwright ë¸Œë¼ìš°ì € ìë™í™”

```python
from playwright.async_api import async_playwright, Browser, Page
import asyncio

class BrowserAutomation:
    """Playwright ë¸Œë¼ìš°ì € ìë™í™”"""

    async def __aenter__(self):
        self.playwright = await async_playwright().start()
        self.browser = await self.playwright.chromium.launch(headless=True)
        return self

    async def __aexit__(self, *args):
        await self.browser.close()
        await self.playwright.stop()

    async def scrape_spa(self, url: str) -> Dict:
        """SPA (Single Page Application) ìŠ¤í¬ë˜í•‘"""
        page = await self.browser.new_page()

        try:
            logger.info("loading_spa", url=url)

            # í˜ì´ì§€ ë¡œë“œ
            await page.goto(url, wait_until='networkidle')

            # JavaScript ë Œë”ë§ ëŒ€ê¸°
            await page.wait_for_selector('.content', timeout=10000)

            # ë°ì´í„° ì¶”ì¶œ
            data = await page.evaluate('''() => {
                return {
                    title: document.querySelector('h1')?.textContent,
                    items: Array.from(document.querySelectorAll('.item')).map(el => ({
                        name: el.querySelector('.name')?.textContent,
                        price: el.querySelector('.price')?.textContent
                    }))
                };
            }''')

            return data

        finally:
            await page.close()

    async def fill_form(self, url: str, form_data: Dict):
        """í¼ ìë™ ì…ë ¥"""
        page = await self.browser.new_page()

        try:
            await page.goto(url)

            # í¼ ì…ë ¥
            await page.fill('input[name="username"]', form_data['username'])
            await page.fill('input[name="password"]', form_data['password'])

            # ì œì¶œ
            await page.click('button[type="submit"]')

            # ê²°ê³¼ ëŒ€ê¸°
            await page.wait_for_url('**/dashboard')

            logger.info("form_submitted", url=page.url)

        finally:
            await page.close()

    async def take_screenshot(self, url: str, output_path: str):
        """ìŠ¤í¬ë¦°ìƒ· ìº¡ì²˜"""
        page = await self.browser.new_page()

        try:
            await page.goto(url, wait_until='networkidle')
            await page.screenshot(path=output_path, full_page=True)

            logger.info("screenshot_saved", path=output_path)

        finally:
            await page.close()

# ì‚¬ìš©
async def main():
    async with BrowserAutomation() as bot:
        # SPA ìŠ¤í¬ë˜í•‘
        data = await bot.scrape_spa("https://app.example.com")

        # í¼ ì œì¶œ
        await bot.fill_form("https://example.com/login", {
            'username': 'user@example.com',
            'password': 'secret'
        })

        # ìŠ¤í¬ë¦°ìƒ·
        await bot.take_screenshot("https://example.com", "screenshot.png")

if __name__ == '__main__':
    asyncio.run(main())
```

---

## íŒŒì¼ ê°ì‹œ ìë™í™”

```mermaid
graph LR
    A[íŒŒì¼ ì‹œìŠ¤í…œ] --> B[watchdog]
    B --> C{ì´ë²¤íŠ¸ ê°ì§€}
    C -->|ìƒì„±| D[íŒŒì¼ ìƒì„±]
    C -->|ìˆ˜ì •| E[íŒŒì¼ ìˆ˜ì •]
    C -->|ì‚­ì œ| F[íŒŒì¼ ì‚­ì œ]

    D --> G[ì²˜ë¦¬ ë¡œì§]
    E --> G
    F --> G

    style B fill:#FF6B6B
    style G fill:#4ECDC4
```

### íŒ¨í„´ 1: íŒŒì¼ ë³€ê²½ ê°ì§€

```python
# tasks/file_watcher.py
from watchdog.observers import Observer
from watchdog.events import FileSystemEventHandler, FileSystemEvent
from pathlib import Path
import time
import structlog

logger = structlog.get_logger()

class FileWatcher(FileSystemEventHandler):
    """íŒŒì¼ ë³€ê²½ ê°ì§€ í•¸ë“¤ëŸ¬"""

    def __init__(self, processor_func=None):
        super().__init__()
        self.processor = processor_func or self.default_processor

    def on_created(self, event: FileSystemEvent):
        """íŒŒì¼ ìƒì„± ì‹œ"""
        if event.is_directory:
            return

        logger.info("file_created", path=event.src_path)
        self.processor(event.src_path, action='created')

    def on_modified(self, event: FileSystemEvent):
        """íŒŒì¼ ìˆ˜ì • ì‹œ"""
        if event.is_directory:
            return

        logger.info("file_modified", path=event.src_path)
        self.processor(event.src_path, action='modified')

    def on_deleted(self, event: FileSystemEvent):
        """íŒŒì¼ ì‚­ì œ ì‹œ"""
        if event.is_directory:
            return

        logger.info("file_deleted", path=event.src_path)

    def default_processor(self, filepath: str, action: str):
        """ê¸°ë³¸ ì²˜ë¦¬ ë¡œì§"""
        logger.info("processing_file", filepath=filepath, action=action)

class AutoProcessor:
    """íŒŒì¼ ìë™ ì²˜ë¦¬"""

    def __init__(self, watch_dir: str, output_dir: str):
        self.watch_dir = Path(watch_dir)
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)

    def process_csv(self, filepath: str, action: str):
        """CSV íŒŒì¼ ì²˜ë¦¬"""
        if not filepath.endswith('.csv'):
            return

        logger.info("processing_csv", filepath=filepath)

        import polars as pl

        # CSV ì½ê¸°
        df = pl.read_csv(filepath)

        # ì²˜ë¦¬ (ì˜ˆ: í•„í„°ë§, ë³€í™˜)
        df_processed = df.filter(pl.col("value") > 0)

        # ê²°ê³¼ ì €ì¥
        output_path = self.output_dir / f"processed_{Path(filepath).name}"
        df_processed.write_csv(output_path)

        logger.info("csv_processed", output=str(output_path), rows=len(df_processed))

    def process_image(self, filepath: str, action: str):
        """ì´ë¯¸ì§€ ì²˜ë¦¬"""
        if not filepath.lower().endswith(('.png', '.jpg', '.jpeg')):
            return

        logger.info("processing_image", filepath=filepath)

        from PIL import Image

        # ì´ë¯¸ì§€ ì—´ê¸°
        img = Image.open(filepath)

        # ë¦¬ì‚¬ì´ì¦ˆ
        img_resized = img.resize((800, 600))

        # ì €ì¥
        output_path = self.output_dir / f"resized_{Path(filepath).name}"
        img_resized.save(output_path)

        logger.info("image_processed", output=str(output_path))

    def start_watching(self):
        """íŒŒì¼ ê°ì‹œ ì‹œì‘"""
        handler = FileWatcher(processor_func=self.process_csv)
        observer = Observer()
        observer.schedule(handler, str(self.watch_dir), recursive=False)
        observer.start()

        logger.info("file_watcher_started", directory=str(self.watch_dir))

        try:
            while True:
                time.sleep(1)
        except KeyboardInterrupt:
            observer.stop()
            logger.info("file_watcher_stopped")

        observer.join()

# ì‚¬ìš©
if __name__ == '__main__':
    processor = AutoProcessor(
        watch_dir='data/input',
        output_dir='data/output'
    )
    processor.start_watching()
```

### íŒ¨í„´ 2: íŒ¨í„´ ê¸°ë°˜ ì²˜ë¦¬

```python
import re
from typing import Callable, Dict

class PatternBasedWatcher:
    """íŒŒì¼ íŒ¨í„´ë³„ ì²˜ë¦¬"""

    def __init__(self, watch_dir: str):
        self.watch_dir = Path(watch_dir)
        self.handlers: Dict[str, Callable] = {}

    def register_handler(self, pattern: str, handler: Callable):
        """íŒ¨í„´ í•¸ë“¤ëŸ¬ ë“±ë¡"""
        self.handlers[pattern] = handler
        logger.info("handler_registered", pattern=pattern)

    def dispatch(self, filepath: str, action: str):
        """íŒ¨í„´ì— ë§ëŠ” í•¸ë“¤ëŸ¬ ì‹¤í–‰"""
        filename = Path(filepath).name

        for pattern, handler in self.handlers.items():
            if re.match(pattern, filename):
                logger.info("handler_matched", pattern=pattern, file=filename)
                handler(filepath, action)
                return

        logger.info("no_handler_matched", file=filename)

# ì‚¬ìš©
watcher = PatternBasedWatcher('data/input')

# í•¸ë“¤ëŸ¬ ë“±ë¡
watcher.register_handler(r'sales_\d{8}\.csv', process_sales_report)
watcher.register_handler(r'invoice_.*\.pdf', process_invoice)
watcher.register_handler(r'log_.*\.txt', process_log_file)

# ê°ì‹œ ì‹œì‘
handler = FileWatcher(processor_func=watcher.dispatch)
observer = Observer()
observer.schedule(handler, 'data/input', recursive=False)
observer.start()
```

---

## ì‘ì—… í ìë™í™”

### Dramatiqìœ¼ë¡œ ë¹„ë™ê¸° ì‘ì—… ì²˜ë¦¬

```python
import dramatiq
from dramatiq.brokers.redis import RedisBroker
import structlog

logger = structlog.get_logger()

# Redis ë¸Œë¡œì»¤ ì„¤ì •
redis_broker = RedisBroker(host="localhost", port=6379)
dramatiq.set_broker(redis_broker)

@dramatiq.actor(max_retries=3, min_backoff=1000, max_backoff=60000)
def send_email(to: str, subject: str, body: str):
    """ì´ë©”ì¼ ì „ì†¡ (ë¹„ë™ê¸°)"""
    logger.info("sending_email", to=to, subject=subject)

    import smtplib
    from email.message import EmailMessage

    msg = EmailMessage()
    msg['Subject'] = subject
    msg['From'] = 'noreply@example.com'
    msg['To'] = to
    msg.set_content(body)

    # SMTP ì „ì†¡
    # with smtplib.SMTP('localhost') as smtp:
    #     smtp.send_message(msg)

    logger.info("email_sent", to=to)

@dramatiq.actor
def process_image_async(filepath: str):
    """ì´ë¯¸ì§€ ì²˜ë¦¬ (ë¹„ë™ê¸°)"""
    logger.info("processing_image_async", filepath=filepath)

    from PIL import Image

    img = Image.open(filepath)
    # ì²˜ë¦¬ ë¡œì§

    logger.info("image_processed_async", filepath=filepath)

@dramatiq.actor
def generate_report(data: dict):
    """ë¦¬í¬íŠ¸ ìƒì„± (ë¹„ë™ê¸°)"""
    logger.info("generating_report", data_keys=list(data.keys()))

    # ë¦¬í¬íŠ¸ ìƒì„± ë¡œì§
    report_path = f"reports/report_{data['id']}.pdf"

    # ì™„ë£Œ í›„ ì´ë©”ì¼ ì „ì†¡ (ì‘ì—… ì²´ì¸)
    send_email.send(
        to=data['email'],
        subject="ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ",
        body=f"ë¦¬í¬íŠ¸ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤: {report_path}"
    )

# ì‘ì—… íì— ì¶”ê°€
send_email.send("user@example.com", "í…ŒìŠ¤íŠ¸", "ë³¸ë¬¸")
process_image_async.send("/path/to/image.jpg")
generate_report.send({'id': 123, 'email': 'user@example.com'})
```

---

## ì•Œë¦¼ ìë™í™”

### íŒ¨í„´ 1: Slack ì•Œë¦¼

```python
# tasks/notifications.py
from slack_sdk import WebClient
from slack_sdk.errors import SlackApiError
import structlog

logger = structlog.get_logger()

class SlackNotifier:
    def __init__(self, token: str, channel: str):
        self.client = WebClient(token=token)
        self.channel = channel

    def send_message(self, text: str, blocks=None):
        """ë©”ì‹œì§€ ì „ì†¡"""
        try:
            response = self.client.chat_postMessage(
                channel=self.channel,
                text=text,
                blocks=blocks
            )
            logger.info("slack_message_sent", channel=self.channel)
            return response

        except SlackApiError as e:
            logger.error("slack_error", error=str(e))

    def send_alert(self, title: str, message: str, severity: str = "info"):
        """ì•Œë¦¼ ì „ì†¡"""
        color = {
            'info': '#36a64f',
            'warning': '#ff9800',
            'error': '#f44336'
        }[severity]

        blocks = [
            {
                "type": "section",
                "text": {
                    "type": "mrkdwn",
                    "text": f"*{title}*\n{message}"
                }
            }
        ]

        self.send_message(text=title, blocks=blocks)

    def send_report(self, title: str, metrics: dict):
        """ë¦¬í¬íŠ¸ ì „ì†¡"""
        fields = [
            {
                "type": "mrkdwn",
                "text": f"*{key}*\n{value}"
            }
            for key, value in metrics.items()
        ]

        blocks = [
            {
                "type": "section",
                "text": {
                    "type": "mrkdwn",
                    "text": f"*{title}*"
                }
            },
            {
                "type": "section",
                "fields": fields
            }
        ]

        self.send_message(text=title, blocks=blocks)

# ì‚¬ìš©
notifier = SlackNotifier(
    token="xoxb-your-token",
    channel="#alerts"
)

# ì•Œë¦¼ ì „ì†¡
notifier.send_alert(
    title="ë°ì´í„° ì²˜ë¦¬ ì™„ë£Œ",
    message="1000ê°œ ë ˆì½”ë“œ ì²˜ë¦¬ ì™„ë£Œ",
    severity="info"
)

# ë¦¬í¬íŠ¸ ì „ì†¡
notifier.send_report(
    title="ì¼ì¼ í†µê³„",
    metrics={
        "ì²˜ë¦¬ëœ íŒŒì¼": "150ê°œ",
        "ì´ ë ˆì½”ë“œ": "50,000ê°œ",
        "ì—ëŸ¬": "0ê°œ"
    }
)
```

---

## í†µí•© ìë™í™” ì‹œìŠ¤í…œ

### ëª¨ë“  ê¸°ëŠ¥ í†µí•©

```python
# main.py
from tasks.scheduler import TaskScheduler
from tasks.file_watcher import AutoProcessor
from tasks.notifications import SlackNotifier
import structlog
from concurrent.futures import ThreadPoolExecutor

logger = structlog.get_logger()

class AutomationSystem:
    """í†µí•© ìë™í™” ì‹œìŠ¤í…œ"""

    def __init__(self, config: dict):
        self.config = config
        self.notifier = SlackNotifier(
            token=config['slack_token'],
            channel=config['slack_channel']
        )

    def start_scheduler(self):
        """ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘"""
        scheduler = TaskScheduler()
        scheduler.start()

    def start_file_watcher(self):
        """íŒŒì¼ ê°ì‹œ ì‹œì‘"""
        processor = AutoProcessor(
            watch_dir=self.config['watch_dir'],
            output_dir=self.config['output_dir']
        )
        processor.start_watching()

    def run(self):
        """ì‹œìŠ¤í…œ ì‹¤í–‰"""
        logger.info("automation_system_starting")

        # ì‹œì‘ ì•Œë¦¼
        self.notifier.send_alert(
            title="ìë™í™” ì‹œìŠ¤í…œ ì‹œì‘",
            message="ëª¨ë“  ì„œë¹„ìŠ¤ ê°€ë™ ì¤‘",
            severity="info"
        )

        # ë³‘ë ¬ ì‹¤í–‰
        with ThreadPoolExecutor(max_workers=2) as executor:
            executor.submit(self.start_scheduler)
            executor.submit(self.start_file_watcher)

# ì‹¤í–‰
if __name__ == '__main__':
    config = {
        'slack_token': 'xoxb-...',
        'slack_channel': '#automation',
        'watch_dir': 'data/input',
        'output_dir': 'data/output'
    }

    system = AutomationSystem(config)
    system.run()
```

---

## ëª¨ë‹ˆí„°ë§ ë° ë¡œê¹…

### structlog ì„¤ì •

```python
# utils/logger.py
import structlog
import logging

def setup_logging():
    structlog.configure(
        processors=[
            structlog.stdlib.add_log_level,
            structlog.processors.TimeStamper(fmt="iso"),
            structlog.processors.JSONRenderer()
        ],
        logger_factory=structlog.stdlib.LoggerFactory(),
    )

    logging.basicConfig(level=logging.INFO)
```

---

## í”„ë¡œë•ì…˜ ë°°í¬

### Docker Compose

```yaml
# docker-compose.yml
version: '3.8'

services:
  automation:
    build: .
    volumes:
      - ./data:/app/data
      - ./logs:/app/logs
    environment:
      - REDIS_URL=redis://redis:6379
      - SLACK_TOKEN=${SLACK_TOKEN}
    depends_on:
      - redis

  dramatiq-worker:
    build: .
    command: dramatiq tasks.workers
    depends_on:
      - redis

  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
```

---

## ë§ˆë¬´ë¦¬

ì—¬ê¸° ë‚˜ì˜¨ íŒ¨í„´ë“¤ì€ ì‹¤ì œë¡œ ìì£¼ ì“°ëŠ” ê²ƒë“¤ì…ë‹ˆë‹¤. APSchedulerë¡œ ìŠ¤ì¼€ì¤„ë§í•˜ê³ , watchdogë¡œ íŒŒì¼ ê°ì‹œí•˜ê³ , Playwrightë¡œ ì›¹ ìŠ¤í¬ë˜í•‘í•˜ëŠ” ê±´ ëŒ€ë¶€ë¶„ì˜ ìë™í™” í”„ë¡œì íŠ¸ì—ì„œ í•„ìš”í•œ ê¸°ëŠ¥ë“¤ì´ì—ìš”.

ì²˜ìŒì—” ê°„ë‹¨í•˜ê²Œ ì‹œì‘í•˜ì„¸ìš”:
- ìŠ¤ì¼€ì¤„ë§ë§Œ í•„ìš”í•˜ë©´ APSchedulerë§Œ ì¨ë„ ì¶©ë¶„
- CLI ë„êµ¬ëŠ” clickì´ typerë³´ë‹¤ ë” ë„ë¦¬ ì“°ì„ (typerëŠ” ë” í˜„ëŒ€ì ì´ê¸´ í•¨)
- ì›¹ ìŠ¤í¬ë˜í•‘ì€ ì¼ë‹¨ httpx + BeautifulSoupë¡œ ì‹œë„í•˜ê³ , JavaScript ë Œë”ë§ì´ í•„ìš”í•˜ë©´ Playwright ë„ì…

ì‘ì—… í(dramatiq)ëŠ” ê·œëª¨ê°€ ì»¤ì§€ë©´ í•„ìš”í•œë°, ì²˜ìŒë¶€í„° ë„£ìœ¼ë©´ ì˜¤íˆë ¤ ë³µì¡í•´ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë‹¨ìˆœí•œ ìŠ¤ì¼€ì¤„ë§ìœ¼ë¡œ ë²„í‹¸ ìˆ˜ ìˆìœ¼ë©´ ê·¸ê²Œ ë” ë‚˜ì•„ìš”.

---

**[â† ë°ì´í„° íŒŒì´í”„ë¼ì¸](data-pipeline.md)** | **[ë¼ì´ë¸ŒëŸ¬ë¦¬ ì¹´íƒˆë¡œê·¸ â†’](../04-library-catalog/README.md)**
