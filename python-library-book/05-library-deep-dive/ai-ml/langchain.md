# LangChain ì™„ë²½ ê°€ì´ë“œ

> **LLM ì• í”Œë¦¬ì¼€ì´ì…˜ ê°œë°œ í”„ë ˆì„ì›Œí¬**

â­ **2026 ì¶”ì²œ** | ğŸ¤– LLM | ğŸ”— ì²´ì¸ | ğŸ§  RAG

---

## ëª©ì°¨

- [ê°œìš”](#ê°œìš”)
- [ì™œ LangChainì¸ê°€](#ì™œ-langchainì¸ê°€)
- [í•µì‹¬ ê°œë…](#í•µì‹¬-ê°œë…)
- [ì„¤ì¹˜ ë° ê¸°ë³¸ ì‚¬ìš©](#ì„¤ì¹˜-ë°-ê¸°ë³¸-ì‚¬ìš©)
- [ì‹¤ì „ íŒ¨í„´ 10ê°€ì§€](#ì‹¤ì „-íŒ¨í„´-10ê°€ì§€)
- [RAG êµ¬í˜„](#rag-êµ¬í˜„)
- [í”„ë¡œë•ì…˜ ê³ ë ¤ì‚¬í•­](#í”„ë¡œë•ì…˜-ê³ ë ¤ì‚¬í•­)

---

## ê°œìš”

### ê¸°ë³¸ ì •ë³´

| í•­ëª© | ë‚´ìš© |
|------|------|
| **ê³µì‹ ì‚¬ì´íŠ¸** | https://python.langchain.com |
| **GitHub** | https://github.com/langchain-ai/langchain |
| **PyPI** | https://pypi.org/project/langchain/ |
| **ì²« ë¦´ë¦¬ì¦ˆ** | 2022ë…„ 10ì›” |
| **ë¼ì´ì„ ìŠ¤** | MIT |

### í•œ ì¤„ ìš”ì•½

**LLMì„ í™œìš©í•œ ì• í”Œë¦¬ì¼€ì´ì…˜ ê°œë°œì„ ìœ„í•œ í†µí•© í”„ë ˆì„ì›Œí¬**

---

## ì™œ LangChainì¸ê°€

### LLM ì§ì ‘ í˜¸ì¶œì˜ ë¬¸ì œ

```python
# OpenAI ì§ì ‘ í˜¸ì¶œ (ë²ˆê±°ë¡œì›€)
import openai

response = openai.ChatCompletion.create(
    model="gpt-4",
    messages=[{"role": "user", "content": "Hello"}],
    temperature=0.7,
)
result = response.choices[0].message.content

# ë¬¸ì œ:
# - í”„ë¡¬í”„íŠ¸ ê´€ë¦¬ ì–´ë ¤ì›€
# - ë¬¸ì„œ ê²€ìƒ‰ í†µí•© ë³µì¡
# - ë©”ëª¨ë¦¬/íˆìŠ¤í† ë¦¬ ê´€ë¦¬ ìˆ˜ë™
# - ì—ëŸ¬ ì²˜ë¦¬ ë°˜ë³µ
```

### LangChain ë°©ì‹

```python
from langchain.chat_models import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.schema.output_parser import StrOutputParser

# ì²´ì¸ êµ¬ì„±
llm = ChatOpenAI(model="gpt-4")
prompt = ChatPromptTemplate.from_template("Tell me about {topic}")
chain = prompt | llm | StrOutputParser()

# ê°„ë‹¨í•œ ì‹¤í–‰
result = chain.invoke({"topic": "Python"})
```

---

## í•µì‹¬ ê°œë…

### 1. ì²´ì¸ (Chains)

```mermaid
graph LR
    A[ì…ë ¥] --> B[í”„ë¡¬í”„íŠ¸]
    B --> C[LLM]
    C --> D[ì¶œë ¥ íŒŒì„œ]
    D --> E[ê²°ê³¼]

    style A fill:#4CAF50
    style E fill:#4CAF50
```

### 2. LCEL (LangChain Expression Language)

```python
# íŒŒì´í”„ ì—°ì‚°ìë¡œ ì²´ì¸ êµ¬ì„±
chain = prompt | llm | parser

# ë™ì¼í•œ ì˜ë¯¸
chain = prompt.chain(llm).chain(parser)
```

### 3. RAG ì•„í‚¤í…ì²˜

```mermaid
graph TB
    A[ì‚¬ìš©ì ì§ˆë¬¸] --> B[ì„ë² ë”© ìƒì„±]
    B --> C[ë²¡í„° DB ê²€ìƒ‰]
    C --> D[ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰]

    D --> E[ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±]
    A --> E
    E --> F[í”„ë¡¬í”„íŠ¸ ìƒì„±]

    F --> G[LLM í˜¸ì¶œ]
    G --> H[ë‹µë³€ ìƒì„±]

    I[ë¬¸ì„œ ë°ì´í„°ë² ì´ìŠ¤] -.-> C

    style A fill:#4CAF50
    style H fill:#4CAF50
    style I fill:#2196F3
```

---

## ì„¤ì¹˜ ë° ê¸°ë³¸ ì‚¬ìš©

### ì„¤ì¹˜

```bash
# ê¸°ë³¸
$ uv add langchain langchain-openai

# RAG ê´€ë ¨
$ uv add langchain-community chromadb

# ë¬¸ì„œ ë¡œë”
$ uv add pypdf python-docx beautifulsoup4
```

### ê¸°ë³¸ ì²´ì¸

```python
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.schema.output_parser import StrOutputParser

# LLM ì´ˆê¸°í™”
llm = ChatOpenAI(
    model="gpt-4-turbo-preview",
    temperature=0,
    api_key="your-api-key"
)

# í”„ë¡¬í”„íŠ¸
prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a helpful assistant."),
    ("user", "{input}")
])

# ì²´ì¸ êµ¬ì„±
chain = prompt | llm | StrOutputParser()

# ì‹¤í–‰
result = chain.invoke({"input": "What is Python?"})
print(result)
```

---

## ì‹¤ì „ íŒ¨í„´ 10ê°€ì§€

### íŒ¨í„´ 1: í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿

```python
from langchain.prompts import PromptTemplate

# ë‹¨ìˆœ í…œí”Œë¦¿
template = """
Translate the following text to {language}:

Text: {text}

Translation:
"""

prompt = PromptTemplate(
    template=template,
    input_variables=["language", "text"]
)

# ì‚¬ìš©
formatted = prompt.format(language="Korean", text="Hello")
```

### íŒ¨í„´ 2: Few-Shot í”„ë¡¬í”„íŠ¸

```python
from langchain.prompts import FewShotPromptTemplate

examples = [
    {"input": "happy", "output": "sad"},
    {"input": "tall", "output": "short"},
]

example_prompt = PromptTemplate(
    input_variables=["input", "output"],
    template="Input: {input}\nOutput: {output}"
)

few_shot_prompt = FewShotPromptTemplate(
    examples=examples,
    example_prompt=example_prompt,
    prefix="Give the antonym of the word",
    suffix="Input: {input}\nOutput:",
    input_variables=["input"]
)
```

### íŒ¨í„´ 3: ë¬¸ì„œ ë¡œë”

```python
from langchain.document_loaders import (
    TextLoader,
    PyPDFLoader,
    DirectoryLoader
)

# ë‹¨ì¼ íŒŒì¼
loader = TextLoader("document.txt")
documents = loader.load()

# PDF
pdf_loader = PyPDFLoader("document.pdf")
pages = pdf_loader.load()

# ë””ë ‰í† ë¦¬ ì „ì²´
dir_loader = DirectoryLoader(
    "docs/",
    glob="**/*.txt",
    loader_cls=TextLoader
)
docs = dir_loader.load()
```

### íŒ¨í„´ 4: í…ìŠ¤íŠ¸ ë¶„í• 

```python
from langchain.text_splitter import RecursiveCharacterTextSplitter

splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,        # ì²­í¬ í¬ê¸°
    chunk_overlap=200,      # ì¤‘ë³µ í¬ê¸°
    length_function=len,
)

splits = splitter.split_documents(documents)
print(f"Split into {len(splits)} chunks")
```

### íŒ¨í„´ 5: ë²¡í„° ìŠ¤í† ì–´

```python
from langchain_openai import OpenAIEmbeddings
from langchain.vectorstores import Chroma

# ì„ë² ë”© ëª¨ë¸
embeddings = OpenAIEmbeddings()

# ë²¡í„° ìŠ¤í† ì–´ ìƒì„±
vectorstore = Chroma.from_documents(
    documents=splits,
    embedding=embeddings,
    persist_directory="./chroma_db"
)

# ê²€ìƒ‰
results = vectorstore.similarity_search("query", k=3)
```

### íŒ¨í„´ 6: RAG ì²´ì¸

```python
from langchain.chains import RetrievalQA

# Retriever ìƒì„±
retriever = vectorstore.as_retriever(
    search_kwargs={"k": 3}
)

# RAG ì²´ì¸
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",  # map_reduce, refine, map_rerank
    retriever=retriever,
    return_source_documents=True
)

# ì§ˆë¬¸
result = qa_chain.invoke({"query": "What is the main topic?"})
print(result["result"])
print(result["source_documents"])
```

### íŒ¨í„´ 7: ëŒ€í™” ë©”ëª¨ë¦¬

```python
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationChain

# ë©”ëª¨ë¦¬ ì´ˆê¸°í™”
memory = ConversationBufferMemory()

# ëŒ€í™” ì²´ì¸
conversation = ConversationChain(
    llm=llm,
    memory=memory,
    verbose=True
)

# ëŒ€í™”
response1 = conversation.predict(input="Hi, I'm Alice")
# "Hello Alice! How can I help you?"

response2 = conversation.predict(input="What's my name?")
# "Your name is Alice"
```

### íŒ¨í„´ 8: ì—ì´ì „íŠ¸

```python
from langchain.agents import initialize_agent, Tool
from langchain.agents import AgentType

# ë„êµ¬ ì •ì˜
def calculator(query):
    return eval(query)

tools = [
    Tool(
        name="Calculator",
        func=calculator,
        description="Useful for math calculations"
    )
]

# ì—ì´ì „íŠ¸ ì´ˆê¸°í™”
agent = initialize_agent(
    tools=tools,
    llm=llm,
    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,
    verbose=True
)

# ì‹¤í–‰
agent.run("What is 25 * 4 + 10?")
```

### íŒ¨í„´ 9: ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ

```python
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler

llm = ChatOpenAI(
    streaming=True,
    callbacks=[StreamingStdOutCallbackHandler()],
    temperature=0
)

# ì‹¤ì‹œê°„ ì¶œë ¥
for chunk in llm.stream("Tell me a story"):
    print(chunk.content, end="", flush=True)
```

### íŒ¨í„´ 10: êµ¬ì¡°í™”ëœ ì¶œë ¥

```python
from langchain.output_parsers import PydanticOutputParser
from pydantic import BaseModel, Field

class Person(BaseModel):
    name: str = Field(description="person's name")
    age: int = Field(description="person's age")
    city: str = Field(description="city where person lives")

parser = PydanticOutputParser(pydantic_object=Person)

prompt = PromptTemplate(
    template="Extract person info from: {text}\n{format_instructions}",
    input_variables=["text"],
    partial_variables={"format_instructions": parser.get_format_instructions()}
)

chain = prompt | llm | parser

result = chain.invoke({"text": "Alice is 30 years old and lives in NYC"})
# Person(name='Alice', age=30, city='NYC')
```

---

## RAG êµ¬í˜„ (ì™„ì „í•œ ì˜ˆì œ)

```python
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain.document_loaders import DirectoryLoader, TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import Chroma
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate

# 1. ë¬¸ì„œ ë¡œë“œ
loader = DirectoryLoader(
    "docs/",
    glob="**/*.txt",
    loader_cls=TextLoader
)
documents = loader.load()

# 2. í…ìŠ¤íŠ¸ ë¶„í• 
splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200
)
splits = splitter.split_documents(documents)

# 3. ë²¡í„° ìŠ¤í† ì–´ ìƒì„±
embeddings = OpenAIEmbeddings()
vectorstore = Chroma.from_documents(
    documents=splits,
    embedding=embeddings,
    persist_directory="./db"
)

# 4. í”„ë¡¬í”„íŠ¸ ì»¤ìŠ¤í„°ë§ˆì´ì§•
template = """
Use the following context to answer the question.
If you don't know, say "I don't know".

Context:
{context}

Question: {question}

Answer:
"""

prompt = PromptTemplate(
    template=template,
    input_variables=["context", "question"]
)

# 5. RAG ì²´ì¸ ìƒì„±
llm = ChatOpenAI(model="gpt-4", temperature=0)

qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=vectorstore.as_retriever(search_kwargs={"k": 3}),
    chain_type_kwargs={"prompt": prompt},
    return_source_documents=True
)

# 6. ì§ˆë¬¸
result = qa_chain.invoke({"query": "What is the main topic?"})

print("Answer:", result["result"])
print("\nSources:")
for doc in result["source_documents"]:
    print(f"- {doc.metadata.get('source', 'Unknown')}")
```

---

## í”„ë¡œë•ì…˜ ê³ ë ¤ì‚¬í•­

### 1. ë¹„ìš© ì¶”ì 

```python
from langchain.callbacks import get_openai_callback

with get_openai_callback() as cb:
    result = chain.invoke({"input": "question"})
    print(f"Total Tokens: {cb.total_tokens}")
    print(f"Total Cost: ${cb.total_cost:.4f}")
```

### 2. ìºì‹±

```python
from langchain.cache import InMemoryCache
from langchain.globals import set_llm_cache

set_llm_cache(InMemoryCache())

# ë™ì¼í•œ ì¿¼ë¦¬ëŠ” ìºì‹œì—ì„œ ê°€ì ¸ì˜´
```

### 3. ì—ëŸ¬ ì²˜ë¦¬

```python
from tenacity import retry, stop_after_attempt, wait_exponential

@retry(
    stop=stop_after_attempt(3),
    wait=wait_exponential(multiplier=1, min=2, max=10)
)
def call_llm_with_retry(chain, input):
    return chain.invoke(input)
```

### 4. í† í° ì œí•œ

```python
import tiktoken

def count_tokens(text, model="gpt-4"):
    encoding = tiktoken.encoding_for_model(model)
    return len(encoding.encode(text))

def truncate_text(text, max_tokens=4000):
    encoding = tiktoken.encoding_for_model("gpt-4")
    tokens = encoding.encode(text)
    if len(tokens) > max_tokens:
        tokens = tokens[:max_tokens]
    return encoding.decode(tokens)
```

---

## LangSmith (ëª¨ë‹ˆí„°ë§)

```python
import os

# LangSmith í™œì„±í™”
os.environ["LANGCHAIN_TRACING_V2"] = "true"
os.environ["LANGCHAIN_API_KEY"] = "your-api-key"
os.environ["LANGCHAIN_PROJECT"] = "my-project"

# ëª¨ë“  ì²´ì¸ ì‹¤í–‰ì´ ìë™ìœ¼ë¡œ ì¶”ì ë¨
result = chain.invoke({"input": "question"})

# LangSmith UIì—ì„œ í™•ì¸:
# - ê° ë‹¨ê³„ë³„ ì‹¤í–‰ ì‹œê°„
# - ì…ë ¥/ì¶œë ¥
# - í† í° ì‚¬ìš©ëŸ‰
# - ì—ëŸ¬ íŠ¸ë ˆì´ì‹±
```

---

## ì•„í‚¤í…ì²˜ ë‹¤ì´ì–´ê·¸ë¨: RAG ì‹œìŠ¤í…œ

```mermaid
graph TB
    subgraph "ì˜¤í”„ë¼ì¸: ë¬¸ì„œ ì¸ë±ì‹±"
        A1[ë¬¸ì„œ ìˆ˜ì§‘] --> A2[í…ìŠ¤íŠ¸ ì¶”ì¶œ]
        A2 --> A3[ì²­í¬ ë¶„í• ]
        A3 --> A4[ì„ë² ë”© ìƒì„±]
        A4 --> A5[ë²¡í„° DB ì €ì¥]
    end

    subgraph "ì˜¨ë¼ì¸: ì§ˆì˜ ì‘ë‹µ"
        B1[ì‚¬ìš©ì ì§ˆë¬¸] --> B2[ì§ˆë¬¸ ì„ë² ë”©]
        B2 --> B3[ìœ ì‚¬ë„ ê²€ìƒ‰]
        A5 -.-> B3
        B3 --> B4[ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰]
        B4 --> B5[ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±]
        B1 --> B5
        B5 --> B6[í”„ë¡¬í”„íŠ¸ ìƒì„±]
        B6 --> B7[LLM í˜¸ì¶œ]
        B7 --> B8[ë‹µë³€ ë°˜í™˜]
    end

    style A5 fill:#2196F3
    style B8 fill:#4CAF50
```

---

## ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤

### 1. í”„ë¡¬í”„íŠ¸ ë²„ì „ ê´€ë¦¬

```python
# prompts.py
PROMPTS = {
    "qa_v1": "Answer based on context: {context}\nQuestion: {question}",
    "qa_v2": "Use the context to answer. Say 'I don't know' if unsure.\n..."
}

# ì‚¬ìš©
prompt = PromptTemplate.from_template(PROMPTS["qa_v2"])
```

### 2. í™˜ê²½ë³„ ì„¤ì •

```python
from pydantic_settings import BaseSettings

class Settings(BaseSettings):
    openai_api_key: str
    model_name: str = "gpt-4"
    temperature: float = 0.0
    max_tokens: int = 2000

settings = Settings()
llm = ChatOpenAI(model=settings.model_name)
```

### 3. êµ¬ì¡°í™”ëœ ë¡œê¹…

```python
import structlog

logger = structlog.get_logger()

def qa_with_logging(query):
    logger.info("qa_start", query=query)

    result = qa_chain.invoke({"query": query})

    logger.info(
        "qa_complete",
        query=query,
        answer_length=len(result["result"]),
        sources_count=len(result["source_documents"])
    )

    return result
```

---

**[â† AI/ML](../../04-library-catalog/machine-learning/README.md)** | **[ë‹¤ìŒ: ChromaDB â†’](chromadb.md)**
